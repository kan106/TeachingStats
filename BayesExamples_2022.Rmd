---
title: "BayesExamples"
author: "Kole Norberg"
date: "4/25/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown
First up is to load in the necessary packages. You may need to install them. brms uses stan which can be a pain to load. Follow these steps:

1. Open the terminal an run xcode-select --install for mac or go here for pc https://github.com/stan-dev/rstan/wiki/Configuring-C---Toolchain-for-Windows
2. If on a Mac with an M1 chip install the pkg file at the bottom of this page https://github.com/fxcoudert/gfortran-for-macOS/releases/tag/11-arm-alpha2
If on a Mac with an intel chip use this page https://github.com/fxcoudert/gfortran-for-macOS/releases/tag/8.2

Your Mac may "refuse" to install. If that happens, "right click" and select open manually. Then the refusal is just a warning and installation will work.

2. Install rstan. You don't need to load it. It just need to be installed. That's true for all of the below packages.

2. Install ps. I don't know what this is, but my fresh build failed because it wasn't installed.

3. Test if it's installed with the example() code provided. If this fails, you'll need to google the error. Don't procede until it's installed correctly. You'll know it works because you get a long chain of code. If you get a warning, that's not a big deal. Warnings mean it's installed. Errors though are a problem.

4. Install checkmate, zoo, and igraph. I did this from a clean install with no packages. You may already have these. I think these are all you need, but if you get an error "there is no package called ___" Just install it and repeat step 5 until it works.

5. Install brms

It might ask you if you want to compile, say no. Compile won't work unless you've set up build tools in r.  Also, start with a clean R environment when you install and load up brms or anything else with stan in it. There are some libraries that can get in the way. 

The other packages I'm suggesting are simstandard to help create the data and BayesFactor for quick computation of bayes factors. Tidyverse I'm assuming you have. 

```{r libraries}

#install.packages("rstan")
#install.packages("ps")
#example(stan_model, package = "rstan", run.dontrun = TRUE)
#install.packages("checkmate")
#install.packages("zoo")
#install.packages("igraph")
#install.packages("brms")
#install.packages("BayesFactor")
#install.packages("simstandard")
library(brms)
#library(tidyverse)
library(simstandard)
library(BayesFactor)
```

##Data simulation
I've created four data sets. Two have 40 observations and two have 400. Priors will affect the results differently depending on the number of observations. Each set (small and large) also have a case in which the data will come out to be sig (p<.05) and a case where they will not (p>.05). Note that in the sig case, the data is standardized (the intercept will therefore be close to 0). I've posted the histograms so you can see how the distributions look and have computed the correlations between the x (age) and y (vocabulary knowledge) variables to verify that the relationship in the significant set is greater than in the non-sig. set.
Using the same random see (set.seed()) will usually ensure you get the same results as I do.
```{r create datasets small}
set.seed(4321)
df.ns.small <- tibble::tibble(vk = rnorm(40,200,50), age=rnorm(40,18,4))
summary(df.ns.small)
hist(df.ns.small$age)
hist(df.ns.small$vk)
cor(df.ns.small$age,df.ns.small$vk) #.02

df.s.small <- simstandard::sim_standardized("age ~~ 0.75 * vk", n = 40)
summary(df.s.small)
hist(df.s.small$age)
hist(df.s.small$vk)
cor(df.s.small$age,df.s.small$vk) #.64
```

```{r create datasets large}
set.seed(4321)
df.ns.large <- tibble::tibble(vk = rnorm(400,200,50), age=rnorm(400,18,4))
summary(df.ns.large)
hist(df.ns.large$age)
hist(df.ns.large$vk)
cor(df.ns.large$age,df.ns.large$vk)#.02

df.s.large <- simstandard::sim_standardized("age ~~ 0.75 * vk", n = 400)
summary(df.s.large)
hist(df.s.large$age)
hist(df.s.large$vk)
cor(df.s.large$age,df.s.large$vk)#.71
```

##Setting priors
I'm going to do everything in brms because I think it's the easiest to work with all things considered. stan_lm() in the rstanarm library is also good. 

I'm setting the priors below to be either flat (basically no prior), diffuse, weak, informative, or hyper informative based on a normal distribution. If you're doing logistic regression, you would actually probably want to set the priors on a beta distribution (not the same beta as the beta coefficient) which can be a bit tricky, so I suggest watching a few videos on the beta distribution if you plan to do logistic regression with brms. The cauchy distribution is also commonly used with continuous data. I'm using the normal here because it's more familiar and therefore easier to understand how the prior affects the posterior, but you'll want to look up the distribution you use to make sure it reflects your prior beliefs.

For the normal distribution, I set priors using the mean and sd. The sd is what determines how informative the prior is. Class refers to either the variable (b - think beta coefficient), the intercept, sigma (error), or random interecepts and slopes if you're using mixed models. Coef is the name of the variable (case sensitive). The priors I'm setting up here suggest a belief that for every increase in age by one year, vocabulary knowledge increased by 5 but that the mean (intercept) for vocabulary knowledge was 200 in the case of raw data or 0 in the case of standardized data. 

Getting a bayes factor requires setting reasonable priors. The way you set the prior can heavily bias your bayes factor. The best way to handle this is to do a sensitivity analysis to see how your bayes factor reacts to different levels of priors. Here we will also look at how it reacts to priors that suggest there is no relationship (null).

Although we're primarily interested in the beta coefficient on the variable, setting a prior for the intercept can also be useful. Here I'm using the student_t because it's normally used for the intercept in default cases. The first number you get is the df, then the mean, then the sd. Because we're mostly interested in the deviation from the intercept, I think it's ok to go with the default prior from brms and have only set the prior on the intercept once as an example.

Notice that there isn't code for flat. Flat is the default. brms calls it a flat distribution of the reals which just means real numbers.

```{r setup priors at different levels}
#Priors for an effect, not standardized
diffuse <- c(set_prior("normal(5,100)", class = "b", coef = "age"),
             set_prior("student_t(3,200,2.5)", class="Intercept"))
weak <- set_prior("normal(5,10)", class = "b", coef = "age")
informative <- set_prior("normal(5,1)", class = "b", coef = "age")
hyperinformative <- set_prior("normal(5,.1)", class = "b", coef = "age")

#Priors for an effect, standardized. I only need to do thie for the diffuse prior because it has a prior on the intercept which will change when standardized
diffuse.s <- c(set_prior("normal(5,100)", class = "b", coef = "age"),
             set_prior("student_t(3,0,2.5)", class="Intercept"))


#Null Priors
diffuseN <- c(set_prior("normal(0,100)", class = "b", coef = "age"),
             set_prior("student_t(3,200,2.5)", class="Intercept"))
diffuse.sN <- c(set_prior("normal(0,100)", class = "b", coef = "age"),
             set_prior("student_t(3,0,2.5)", class="Intercept"))
weakN <- set_prior("normal(0,10)", class = "b", coef = "age")
informativeN <- set_prior("normal(0,1)", class = "b", coef = "age")
hyperinformativeN <- set_prior("normal(0,.1)", class = "b", coef = "age")
```

##Running brms
brms is set up just like the lm with a few other parameters. family=gaussian() says that this is a continuous variable and we are using a normal distribution. If this were logistic, I would use family=bernoulli (not binomial). The math for these distributions is different but for inferential purposes, they produce the same results. Brms prefers bernoulli. The bernoulli is just a special case of the binomial.

Chains refers to the markov chains. 4 is plenty. Explaining this is beyond the scope of this document. Basically, the model starts in 4 places on the distribution and checks to see if it ends up in the same spot each time. rhat=1 in the summary output if this works. 

iter refers to the number of iterations in each chain. 2000 is usually enough, but if you find that in the summary output, rhat does not = 1, you should increase your iterations as you don't have a good fit. rhat = 1.01 is not good enough; you want 1. 2000 is the default, so I have it set once in ns.s.flat so you can see how to do it, but I don't set it again. 

cores allows you to use your computers processing power to speed up the computations. You can use as many cores as there are chains provided your computer has that many. Each core will run a chain. I have 8 cores on my computer, but I can only use 4 because there are only 4 chains. If you have 4 cores, you could try running it using all 4 cores but just know that your computer may be drastically slowed down by this because you are engaging all of your processing power. That said, bayesian models can take a long time to run, especially if you start dealing with random effects so sometimes its worth it to engage everything you've got. 

sample_prior = TRUE allows us to plot our priors later. It's just telling brms not to trash this data. If you're having trouble understanding your prior, plotting it is the easiest way to get a sense of what is happening. This only works though if you set priors; it doesn't work with brms default priors.

save_all_pars = save_pars(all=TRUE) keeps the appropriate values on hand for calculating the bayes factor.

I'm running six models below. the first is the basic linear model, then a brms model with flat priors. Flat priors are set by brms. They are uninformative and improper (don't sum to 1) priors and consist basically of placing equal probability on all real numbers. We then move on to the diffuse, weakly informative, informative, and hyperinformative models. The next output consists of the prior summaries followed by the results summaries. Results summaries are similar to lm summaries with a few small differences that we'll go over in class. The final bit are the plots of the brms models. Note that the flat model has no prior distribution plotted.

```{r test the effect with small ns data set}
#Basic linear model
mod.lm.ns.small <- lm(vk~age,data=df.ns.small)

#brms with flat priors (brms sets a prior here that is flat but over the reals meaning only covers the real numbers)
ns.s.flat <- brm(vk~age,data=df.ns.small,family=gaussian(),chains=4,iter=2000,cores=4,sample_prior = TRUE,save_pars=save_pars(all=TRUE))
#brms with minimal priors
ns.s.diffuse <- brm(vk~age,data=df.ns.small,family=gaussian(),chains=4,cores=4,prior=diffuse,sample_prior = TRUE,save_pars=save_pars(all=TRUE))
#brms with weak priors
ns.s.weak <- brm(vk~age,data=df.ns.small,family=gaussian(),chains=4,cores=4,prior=weak,sample_prior = TRUE,save_pars=save_pars(all=TRUE))
#brms with informative priors
ns.s.inf <- brm(vk~age,data=df.ns.small,family=gaussian(),chains=4,cores=4,prior=informative,sample_prior = TRUE,save_pars=save_pars(all=TRUE))
#brms with hyper informative priors
ns.s.hyper <- brm(vk~age,data=df.ns.small,family=gaussian(),chains=4,cores=4,prior=hyperinformative,sample_prior = TRUE,save_pars=save_pars(all=TRUE))
```

```{r output for brms ns small data}
prior_summary(ns.s.flat)
prior_summary(ns.s.diffuse)
prior_summary(ns.s.weak)
prior_summary(ns.s.inf)
prior_summary(ns.s.hyper)
summary(mod.lm.ns.small)
summary(ns.s.flat)
summary(ns.s.diffuse)
summary(ns.s.weak)
summary(ns.s.inf)
summary(ns.s.hyper)
plot(hypothesis(ns.s.flat, "age = 0"),plot=FALSE)[[1]] + theme_bw() + labs(title="Flat Priors",y="Density",x="Beta Distribution") + theme(plot.title=element_text(size=18),axis.title=element_text(size=14),axis.text=element_text(size=12), legend.text=element_text(size=14),legend.title=element_text(size=14))+xlim(-10,10)
plot(hypothesis(ns.s.diffuse, "age = 0"),plot=FALSE)[[1]] + theme_bw() + labs(title="Diffuse Priors",y="Density",x="Beta Distribution") + theme(plot.title=element_text(size=18),axis.title=element_text(size=14),axis.text=element_text(size=12), legend.text=element_text(size=14),legend.title=element_text(size=14))+xlim(-10,10)
plot(hypothesis(ns.s.weak, "age = 0"),plot=FALSE)[[1]] + theme_bw() + labs(title="Weak Priors",y="Density",x="Beta Distribution") + theme(plot.title=element_text(size=18),axis.title=element_text(size=14),axis.text=element_text(size=12), legend.text=element_text(size=14),legend.title=element_text(size=14))+xlim(-10,10)
plot(hypothesis(ns.s.inf, "age = 0"),plot=FALSE)[[1]] + theme_bw() + labs(title="Informative Priors",y="Density",x="Beta Distribution") + theme(plot.title=element_text(size=18),axis.title=element_text(size=14),axis.text=element_text(size=12), legend.text=element_text(size=14),legend.title=element_text(size=14))+xlim(-10,10)
plot(hypothesis(ns.s.hyper, "age = 0"),plot=FALSE)[[1]] + theme_bw() + labs(title="Hyper Informative Priors",y="Density",x="Beta Distribution") + theme(plot.title=element_text(size=18),axis.title=element_text(size=14),axis.text=element_text(size=12), legend.text=element_text(size=14),legend.title=element_text(size=14))+xlim(-10,10)
```

The below graphs just put the diffuse, weak, and informative models together into one graph so you can see how the prior distribution affects the posterior. 
```{r combined graph for small ns data}
posterior1 <- posterior_samples(ns.s.diffuse, pars = "b_age")[,c(1,2)]
posterior2 <- posterior_samples(ns.s.weak, pars = "b_age")[,c(1,2)]
posterior3 <- posterior_samples(ns.s.inf, pars = "b_age")[,c(1,2)]

posterior1.2.3 <- bind_rows("prior 1" = pivot_longer(posterior1,c(prior_b_age,b_age)), 
                            "prior 2" = pivot_longer(posterior2,c(prior_b_age,b_age)), 
                            "prior 3" = pivot_longer(posterior3,c(prior_b_age,b_age)), 
                            .id = "id")
modelLME <- lm(vk ~ 1 + age, data = df.ns.small)

ggplot(data    = posterior1.2.3, 
       mapping = aes(x        = value,
                     fill     =  id, 
                     colour   = name,
                     linetype = name, 
                     alpha    = name
                   )) +
  geom_density(size = 1.2)+
  geom_vline(xintercept = summary(modelLME)$coefficients["age", "Estimate"],  #add the frequentist solution too
             size = .8, linetype = 2, col = "black")+ 
  scale_x_continuous(limits = c(-20, 20))+
  coord_cartesian(ylim = c(0, .5))+
  scale_fill_manual(name   = "Densities", 
                    values = c("Yellow","darkred","blue" ), 
                    labels = c("diffuse ~ N(5,100) prior",
                               "weak ~ N(5,10) prior",
                               "informative ~ N(5, 1) prior") )+
  scale_colour_manual(name   = 'Posterior/Prior', 
                      values = c("black","red"), 
                      labels = c("posterior", "prior"))+
  scale_linetype_manual(name   ='Posterior/Prior', 
                        values = c("solid","dotted"), 
                        labels = c("posterior", "prior"))+
  scale_alpha_discrete(name   = 'Posterior/Prior', 
                       range  = c(.7,.3), 
                       labels = c("posterior", "prior"))+
  annotate(geom    = "text", 
           x = 0.45, y = -.13,
           label  = "LME estimate:  0.804", 
           col    = "black", 
           family = theme_get()$text[["family"]], 
           size   = theme_get()$text[["size"]]/3.5, 
           fontface="italic")+
  labs(title    = expression("Influence of Priors on Posterior"),
       subtitle = "3 different densities of priors and posteriors and the LME estimate")+
  theme_bw()
```

We will do all of the above again, except this time for the data set we know is significant. I'm now using include = FALSE in the header in order to suppress the long progress output within the markdown file.
```{r test the effect with small sig data set, include=FALSE}
mod.lm.s.small <- lm(vk~age,data=df.s.small)
summary(mod.lm.s.small)
s.s.flat <- brm(vk~age,data=df.s.small,family=gaussian(),chains=4,cores=2,sample_prior = TRUE,save_pars=save_pars(all=TRUE))
s.s.diffuse <- brm(vk~age,data=df.s.small,family=gaussian(),chains=4,cores=2,prior=diffuse.s,sample_prior = TRUE,save_pars=save_pars(all=TRUE))
s.s.weak <- brm(vk~age,data=df.s.small,family=gaussian(),chains=4,cores=2,prior=weak,sample_prior = TRUE,save_pars=save_pars(all=TRUE))

s.s.inf <- brm(vk~age,data=df.s.small,family=gaussian(),chains=4,cores=2,prior=informative,sample_prior = TRUE,save_pars=save_pars(all=TRUE))
bf.s.inf <- bayes_factor(s.s.hyper2,s.s.inf)
summary(s.s.inf)

s.s.hyper2<- brm(vk~1,data=df.s.small,family=gaussian(),chains=4,cores=2,sample_prior = TRUE,save_pars=save_pars(all=TRUE))
s.s.hyper3<- brm(vk~age,data=df.s.small,family=gaussian(),chains=4,cores=2,prior=hyperinformative,sample_prior = TRUE,save_pars=save_pars(all=TRUE))
s.s.hyper4<- brm(vk~age,data=df.s.small,family=gaussian(),chains=4,cores=2,prior=hyperinformative,sample_prior = TRUE,save_pars=save_pars(all=TRUE))
summary(s.s.hyper4)
bf.s.hyper <- bayes_factor(s.s.hyper2,s.s.hyper) #9.56, 9.52 (evidence for H0)
bf.s.hyper3 <- bayes_factor(s.s.hyper3,s.s.hyper2) #9.56, 9.52 (evidence for H0)
bf.s.hyper4 <- bayes_factor(s.s.hyper4,s.s.hyper2) #9.56, 9.52 (evidence for H0)

```

```{r output for sig small data}
summary(mod.lm.s.small)
summary(s.s.flat)
summary(s.s.diffuse)
summary(s.s.weak)
summary(s.s.inf)
summary(s.s.hyper)
plot(hypothesis(s.s.flat, "age > 0"),plot=FALSE)[[1]] + theme_bw() + labs(title="Flat Priors",y="Density",x="Beta Distribution") + theme(plot.title=element_text(size=18),axis.title=element_text(size=14),axis.text=element_text(size=12), legend.text=element_text(size=14),legend.title=element_text(size=14))+xlim(-10,10)
plot(hypothesis(s.s.diffuse, "age > 0"),plot=FALSE)[[1]] + theme_bw() + labs(title="Diffuse Priors",y="Density",x="Beta Distribution") + theme(plot.title=element_text(size=18),axis.title=element_text(size=14),axis.text=element_text(size=12), legend.text=element_text(size=14),legend.title=element_text(size=14))+xlim(-10,10)
plot(hypothesis(s.s.weak, "age > 0"),plot=FALSE)[[1]] + theme_bw() + labs(title="Weak Priors",y="Density",x="Beta Distribution") + theme(plot.title=element_text(size=18),axis.title=element_text(size=14),axis.text=element_text(size=12), legend.text=element_text(size=14),legend.title=element_text(size=14))+xlim(-10,10)
plot(hypothesis(s.s.inf, "age > 0"),plot=FALSE)[[1]] + theme_bw() + labs(title="Informative Priors",y="Density",x="Beta Distribution") + theme(plot.title=element_text(size=18),axis.title=element_text(size=14),axis.text=element_text(size=12), legend.text=element_text(size=14),legend.title=element_text(size=14))+xlim(-10,10)
plot(hypothesis(s.s.hyper, "age > 0"),plot=FALSE)[[1]] + theme_bw() + labs(title="Hyper Informative Priors",y="Density",x="Beta Distribution") + theme(plot.title=element_text(size=18),axis.title=element_text(size=14),axis.text=element_text(size=12), legend.text=element_text(size=14),legend.title=element_text(size=14))+xlim(-10,10)
```

```{r combined graph for small sig data}
posterior1 <- posterior_samples(s.s.diffuse, pars = "b_age")[,c(1,2)]
posterior2 <- posterior_samples(s.s.weak, pars = "b_age")[,c(1,2)]
posterior3 <- posterior_samples(s.s.inf, pars = "b_age")[,c(1,2)]

posterior1.2.3 <- bind_rows("prior 1" = pivot_longer(posterior1,c(prior_b_age,b_age)), 
                            "prior 2" = pivot_longer(posterior2,c(prior_b_age,b_age)), 
                            "prior 3" = pivot_longer(posterior3,c(prior_b_age,b_age)), 
                            .id = "id")
modelLME <- lm(vk ~ 1 + age, data = df.s.small)

ggplot(data    = posterior1.2.3, 
       mapping = aes(x        = value,
                     fill     =  id, 
                     colour   = name,
                     linetype = name, 
                     alpha    = name
                   )) +
  geom_density(size = 1.2)+
  geom_vline(xintercept = summary(modelLME)$coefficients["age", "Estimate"],  #add the frequentist solution too
             size = .8, linetype = 2, col = "black")+ 
  scale_x_continuous(limits = c(-20, 20))+
  coord_cartesian(ylim = c(0, .5))+
  scale_fill_manual(name   = "Densities", 
                    values = c("Yellow","darkred","blue" ), 
                    labels = c("diffuse ~ N(5,100) prior",
                               "weak ~ N(5,10) prior",
                               "informative ~ N(5, 1) prior") )+
  scale_colour_manual(name   = 'Posterior/Prior', 
                      values = c("black","red"), 
                      labels = c("posterior", "prior"))+
  scale_linetype_manual(name   ='Posterior/Prior', 
                        values = c("solid","dotted"), 
                        labels = c("posterior", "prior"))+
  scale_alpha_discrete(name   = 'Posterior/Prior', 
                       range  = c(.7,.3), 
                       labels = c("posterior", "prior"))+
  annotate(geom    = "text", 
           x = 0.45, y = -.13,
           label  = "LME estimate:  0.804", 
           col    = "black", 
           family = theme_get()$text[["family"]], 
           size   = theme_get()$text[["size"]]/3.5, 
           fontface="italic")+
  labs(title    = expression("Influence of Priors on Posterior"),
       subtitle = "3 different densities of priors and posteriors and the LME estimate")+
  theme_bw()
```
We're repeating the above again for large not significant data set
```{r test the effect with large ns data set,include=FALSE}
mod.lm.ns.large <- lm(vk~age,data=df.ns.large)
l.ns.flat <- brm(vk~age,data=df.ns.large,family=gaussian(),chains=4,cores=2,sample_prior = TRUE,save_pars=save_pars(all=TRUE),silent=2)
l.ns.diffuse <- brm(vk~age,data=df.ns.large,family=gaussian(),chains=4,cores=2,prior=diffuse,sample_prior = TRUE,save_pars=save_pars(all=TRUE))
l.ns.weak <- brm(vk~age,data=df.ns.large,family=gaussian(),chains=4,cores=2,prior=weak,sample_prior = TRUE,save_pars=save_pars(all=TRUE))
l.ns.inf <- brm(vk~age,data=df.ns.large,family=gaussian(),chains=4,cores=2,prior=informative,sample_prior = TRUE,save_pars=save_pars(all=TRUE))
l.ns.hyper <- brm(vk~age,data=df.ns.large,family=gaussian(),chains=4,cores=2,prior=hyperinformative,sample_prior = TRUE,save_pars=save_pars(all=TRUE))
```

```{r output for large ns data}
summary(mod.lm.ns.large)
summary(l.ns.flat)
summary(l.ns.diffuse)
summary(l.ns.weak)
summary(l.ns.inf)
summary(l.ns.hyper)
plot(hypothesis(l.ns.flat, "age > 0"),plot=FALSE)[[1]] + theme_bw() + labs(title="Flat Priors",y="Density",x="Beta Distribution") + theme(plot.title=element_text(size=18),axis.title=element_text(size=14),axis.text=element_text(size=12), legend.text=element_text(size=14),legend.title=element_text(size=14))+xlim(-10,10)
plot(hypothesis(l.ns.diffuse, "age > 0"),plot=FALSE)[[1]] + theme_bw() + labs(title="Diffuse Priors",y="Density",x="Beta Distribution") + theme(plot.title=element_text(size=18),axis.title=element_text(size=14),axis.text=element_text(size=12), legend.text=element_text(size=14),legend.title=element_text(size=14))+xlim(-10,10)
plot(hypothesis(l.ns.weak, "age > 0"),plot=FALSE)[[1]] + theme_bw() + labs(title="Weak Priors",y="Density",x="Beta Distribution") + theme(plot.title=element_text(size=18),axis.title=element_text(size=14),axis.text=element_text(size=12), legend.text=element_text(size=14),legend.title=element_text(size=14))+xlim(-10,10)
plot(hypothesis(l.ns.inf, "age > 0"),plot=FALSE)[[1]] + theme_bw() + labs(title="Informative Priors",y="Density",x="Beta Distribution") + theme(plot.title=element_text(size=18),axis.title=element_text(size=14),axis.text=element_text(size=12), legend.text=element_text(size=14),legend.title=element_text(size=14))+xlim(-10,10)
plot(hypothesis(l.ns.hyper, "age > 0"),plot=FALSE)[[1]] + theme_bw() + labs(title="Hyper Informative Priors",y="Density",x="Beta Distribution") + theme(plot.title=element_text(size=18),axis.title=element_text(size=14),axis.text=element_text(size=12), legend.text=element_text(size=14),legend.title=element_text(size=14))+xlim(-10,10)
```

```{r combined graph for large ns data}
posterior1 <- posterior_samples(l.ns.diffuse, pars = "b_age")[,c(1,2)]
posterior2 <- posterior_samples(l.ns.weak, pars = "b_age")[,c(1,2)]
posterior3 <- posterior_samples(l.ns.inf, pars = "b_age")[,c(1,2)]

posterior1.2.3 <- bind_rows("prior 1" = pivot_longer(posterior1,c(prior_b_age,b_age)), 
                            "prior 2" = pivot_longer(posterior2,c(prior_b_age,b_age)), 
                            "prior 3" = pivot_longer(posterior3,c(prior_b_age,b_age)), 
                            .id = "id")
modelLME <- lm(vk ~ 1 + age, data = df.ns.large)

ggplot(data    = posterior1.2.3, 
       mapping = aes(x        = value,
                     fill     =  id, 
                     colour   = name,
                     linetype = name, 
                     alpha    = name
                   )) +
  geom_density(size = 1.2)+
  geom_vline(xintercept = summary(modelLME)$coefficients["age", "Estimate"],  #add the frequentist solution too
             size = .8, linetype = 2, col = "black")+ 
  scale_x_continuous(limits = c(-20, 20))+
  coord_cartesian(ylim = c(0, 1))+
  scale_fill_manual(name   = "Densities", 
                    values = c("Yellow","darkred","blue" ), 
                    labels = c("diffuse ~ N(5,100) prior",
                               "weak ~ N(5,10) prior",
                               "informative ~ N(5, 1) prior") )+
  scale_colour_manual(name   = 'Posterior/Prior', 
                      values = c("black","red"), 
                      labels = c("posterior", "prior"))+
  scale_linetype_manual(name   ='Posterior/Prior', 
                        values = c("solid","dotted"), 
                        labels = c("posterior", "prior"))+
  scale_alpha_discrete(name   = 'Posterior/Prior', 
                       range  = c(.7,.3), 
                       labels = c("posterior", "prior"))+
  annotate(geom    = "text", 
           x = 0.45, y = -.13,
           label  = "LME estimate:  0.804", 
           col    = "black", 
           family = theme_get()$text[["family"]], 
           size   = theme_get()$text[["size"]]/3.5, 
           fontface="italic")+
  labs(title    = expression("Influence of Priors on Posterior"),
       subtitle = "3 different densities of priors and posteriors and the LME estimate")+
  theme_bw()
```
and finally again with the large sig data set
```{r test the effect with large sig data set, include=FALSE}
mod.lm.s.large <- lm(vk~age,data=df.s.large)
l.s.flat <- brm(vk~age,data=df.s.large,family=gaussian(),chains=4,cores=2,sample_prior = TRUE,save_pars=save_pars(all=TRUE))
l.s.diffuse <- brm(vk~age,data=df.s.large,family=gaussian(),chains=4,cores=2,prior=diffuse.s,sample_prior = TRUE,save_pars=save_pars(all=TRUE))
l.s.weak <- brm(vk~age,data=df.s.large,family=gaussian(),chains=4,cores=2,prior=weak,sample_prior = TRUE,save_pars=save_pars(all=TRUE))
l.s.inf <- brm(vk~age,data=df.s.large,family=gaussian(),chains=4,cores=2,prior=informative,sample_prior = TRUE,save_pars=save_pars(all=TRUE))
l.s.hyper <- brm(vk~age,data=df.s.large,family=gaussian(),chains=4,cores=2,prior=hyperinformative,sample_prior = TRUE,save_pars=save_pars(all=TRUE))
```

```{r output for large sig data}
summary(mod.lm.s.large)
summary(l.s.flat)
summary(l.s.diffuse)
summary(l.s.weak)
summary(l.s.inf)
summary(l.s.hyper)
plot(hypothesis(l.s.flat, "age > 0"),plot=FALSE)[[1]] + theme_bw() + labs(title="Flat Priors",y="Density",x="Beta Distribution") + theme(plot.title=element_text(size=18),axis.title=element_text(size=14),axis.text=element_text(size=12), legend.text=element_text(size=14),legend.title=element_text(size=14))+xlim(-10,10)
plot(hypothesis(l.s.diffuse, "age > 0"),plot=FALSE)[[1]] + theme_bw() + labs(title="Diffuse Priors",y="Density",x="Beta Distribution") + theme(plot.title=element_text(size=18),axis.title=element_text(size=14),axis.text=element_text(size=12), legend.text=element_text(size=14),legend.title=element_text(size=14))+xlim(-10,10)
plot(hypothesis(l.s.weak, "age > 0"),plot=FALSE)[[1]] + theme_bw() + labs(title="Weak Priors",y="Density",x="Beta Distribution") + theme(plot.title=element_text(size=18),axis.title=element_text(size=14),axis.text=element_text(size=12), legend.text=element_text(size=14),legend.title=element_text(size=14))+xlim(-10,10)
plot(hypothesis(l.ns.inf, "age > 0"),plot=FALSE)[[1]] + theme_bw() + labs(title="Informative Priors",y="Density",x="Beta Distribution") + theme(plot.title=element_text(size=18),axis.title=element_text(size=14),axis.text=element_text(size=12), legend.text=element_text(size=14),legend.title=element_text(size=14))+xlim(-10,10)
plot(hypothesis(l.s.hyper, "age > 0"),plot=FALSE)[[1]] + theme_bw() + labs(title="Hyper Informative Priors",y="Density",x="Beta Distribution") + theme(plot.title=element_text(size=18),axis.title=element_text(size=14),axis.text=element_text(size=12), legend.text=element_text(size=14),legend.title=element_text(size=14))+xlim(-10,10)

```

```{r combined graph for large sig data}
posterior1 <- posterior_samples(l.s.diffuse, pars = "b_age")[,c(1,2)]
posterior2 <- posterior_samples(l.s.weak, pars = "b_age")[,c(1,2)]
posterior3 <- posterior_samples(l.s.inf, pars = "b_age")[,c(1,2)]

posterior1.2.3 <- bind_rows("prior 1" = pivot_longer(posterior1,c(prior_b_age,b_age)), 
                            "prior 2" = pivot_longer(posterior2,c(prior_b_age,b_age)), 
                            "prior 3" = pivot_longer(posterior3,c(prior_b_age,b_age)), 
                            .id = "id")
modelLME <- lm(vk ~ 1 + age, data = df.s.large)

ggplot(data    = posterior1.2.3, 
       mapping = aes(x        = value,
                     fill     =  id, 
                     colour   = name,
                     linetype = name, 
                     alpha    = name
                   )) +
  geom_density(size = 1.2)+
  geom_vline(xintercept = summary(modelLME)$coefficients["age", "Estimate"],  #add the frequentist solution too
             size = .8, linetype = 2, col = "black")+ 
  scale_x_continuous(limits = c(-20, 20))+
  coord_cartesian(ylim = c(0, 1))+
  scale_fill_manual(name   = "Densities", 
                    values = c("Yellow","darkred","blue" ), 
                    labels = c("diffuse ~ N(5,100) prior",
                               "weak ~ N(5,10) prior",
                               "informative ~ N(5, 1) prior") )+
  scale_colour_manual(name   = 'Posterior/Prior', 
                      values = c("black","red"), 
                      labels = c("posterior", "prior"))+
  scale_linetype_manual(name   ='Posterior/Prior', 
                        values = c("solid","dotted"), 
                        labels = c("posterior", "prior"))+
  scale_alpha_discrete(name   = 'Posterior/Prior', 
                       range  = c(.7,.3), 
                       labels = c("posterior", "prior"))+
  annotate(geom    = "text", 
           x = 0.45, y = -.13,
           label  = "LME estimate:  0.804", 
           col    = "black", 
           family = theme_get()$text[["family"]], 
           size   = theme_get()$text[["size"]]/3.5, 
           fontface="italic")+
  labs(title    = expression("Influence of Priors on Posterior"),
       subtitle = "3 different densities of priors and posteriors and the LME estimate")+
  theme_bw()
```

```{r combined graph for small large ns data}
posterior1 <- posterior_samples(ns.s.weak, pars = "b_age")[,c(1,2)]
posterior2 <- posterior_samples(l.ns.weak, pars = "b_age")[,c(1,2)]
posterior3 <- posterior_samples(ns.s.inf, pars = "b_age")[,c(1,2)]
posterior4 <- posterior_samples(l.ns.inf, pars = "b_age")[,c(1,2)]

posterior1.2.3.4 <- bind_rows("prior 1" = pivot_longer(posterior1,c(prior_b_age,b_age)), 
                            "prior 2" = pivot_longer(posterior2,c(prior_b_age,b_age)), 
                            "prior 3" = pivot_longer(posterior3,c(prior_b_age,b_age)),
                            "prior 4" =pivot_longer(posterior4,c(prior_b_age,b_age)),
                            .id = "id")
modelLME.l <- lm(vk ~ 1 + age, data = df.ns.large)
modelLME.s <- lm(vk ~ 1 + age, data = df.ns.small)

ggplot(data    = posterior1.2.3.4, 
       mapping = aes(x        = value,
                     fill     =  id, 
                     colour   = name,
                     linetype = name, 
                     alpha    = name
                   )) +
  geom_density(size = 1.2)+
  geom_vline(xintercept = summary(modelLME.s)$coefficients["age", "Estimate"],  #add the frequentist solution too
             size = .8, linetype = 2, col = "grey")+ 
   geom_vline(xintercept = summary(modelLME.l)$coefficients["age", "Estimate"],  #add the frequentist solution too
             
             size = .8, linetype = 2, col = "black")+ 
  scale_x_continuous(limits = c(-10, 10))+
  coord_cartesian(ylim = c(0, .75))+
  scale_fill_manual(name   = "Densities", 
                    values = c("blue","red","green","orange" ), 
                    labels = c("small.ns.weak ~ N(5,10) prior",
                               "large.ns.weak ~ N(5,10) prior",
                               "small.ns.inf ~ N(5, 1) prior",
                    "large.ns.inf ~ N(5, 1) prior"))+
  scale_colour_manual(name   = 'Posterior/Prior', 
                      values = c("black","purple"), 
                      labels = c("posterior", "prior"))+
  scale_linetype_manual(name   ='Posterior/Prior', 
                        values = c("solid","dotted"), 
                        labels = c("posterior", "prior"))+
  scale_alpha_discrete(name   = 'Posterior/Prior', 
                       range  = c(.7,.3), 
                       labels = c("posterior", "prior"))+
  labs(title    = "Influence of Weak and Informative Priors",
       subtitle = "using a small and large data set")+
  theme_bw()
```
Below are just null models for each data set. These can be used to get the Bayes factors
```{r null models for comparison, include=FALSE}
null.small.ns <- brm(vk~1,data=df.ns.small,chains=4,family=gaussian(),cores=2,save_pars=save_pars(all=TRUE)) 
#Our diffuse model has priors set on the intercept. We need to carry these priors forward to the model. They don't affect the bayes factors except that we don't want to compare models with different priors on the alternate variables. So here we use the brms update function to update the formula to include all variables (.) except age (-age).
null.small.ns.diffuse <- update(ns.s.diffuse, formula = ~ .-age)
null.large.ns <- brm(vk~1,data=df.ns.large,chains=4,family=gaussian(),cores=2,save_pars=save_pars(all=TRUE)) 
null.large.ns.diffuse <- update(l.ns.diffuse, formula = ~ .-age)
null.small.s <- brm(vk~1,data=df.s.small,chains=4,family=gaussian(),cores=2,save_pars=save_pars(all=TRUE)) 
null.small.s.diffuse <- update(s.s.diffuse, formula = ~ .-age)
null.large.s <- brm(vk~1,data=df.s.large,chains=4,family=gaussian(),cores=2,save_pars=save_pars(all=TRUE)) 
null.large.s.diffuse <- update(l.s.diffuse, formula = ~ .-age)
```

Now we'll compute the actual bayes factors. I'll walk through two ways to do this but there are others.

The first way involves the library BayesFactor. For this data, we'll use the function lmBF but you'll want to explore to see if that is appropriate for your data. lmBF gives us the evidence for the alternative versus a null model. There are other comparisons that can be set up. You'll need to look into the documentation for your specific needs. A nice feature of lmBF is that it provides an error interval on the BF so you don't need to run it multiple times to see how reliable it is.

Another way to get a BF is through brms native function bayes_factor. For this, you need to run your null model and then you compare the two. Whichever one goes into the parentheses first is the numerator and the BF you return will be the support for that model. Because the bayes_factor function does not give an error interval, I usually run a few times (here just twice) to ensure the results are equivalent across runs. If they're not, I increase the iterations in the original model. I then report the average as the BF and the extent of variation as a range or percentage.

lmBF gives the evidence for the alternative. I set bayes_factor to give the evidence for the null. To get values the other way around, just take 1/BF. 
```{r compute bayes factors}
#Small n.s. data
bf.s.ns.flat <- lmBF(vk~age,df.ns.small) #.31 (evidence for H1)
bf.s.ns.diffuse <- bayes_factor(null.small.ns.diffuse,ns.s.diffuse) #45.32, 45.93 (evidence for H0)
bf.s.ns.weak <- bayes_factor(null.small.ns,ns.s.weak) #5.18, 5.15 (evidence for H0)
bf.s.ns.inf <- bayes_factor(null.small.ns,ns.s.inf) #7.23, 7.26 (evidence for H0)
bf.s.ns.hyper <- bayes_factor(null.small.ns,ns.s.hyper) #9.56, 9.52 (evidence for H0)

#small sig data
bf.s.s.flat <- lmBF(vk~age,df.s.small) #1719.149 (evidence for H1)
bf.s.s.diffuse <- bayes_factor(null.small.s.diffuse,s.s.diffuse) #.04364, .044 (evidence for H0)
bf.s.s.weak <- bayes_factor(null.small.s,s.s.weak) #.0048, .00484 (evidence for H0)
bf.s.s.inf <- bayes_factor(null.small.s,s.s.inf) #8.67, 8.65 (evidence for H0)
bf.s.s.hyper <- bayes_factor(null.small.s,s.s.hyper) #>10000,>10000 (evidence for H0)

#large ns data 
bf.l.ns.flat <- lmBF(vk~age,df.ns.large) #.12 (evidence for H1)
bf.l.ns.diffuse <- bayes_factor(null.large.ns.diffuse,l.ns.diffuse) #150.58, 150.17 (evidence for H0)
bf.l.ns.weak <- bayes_factor(null.large.ns,l.ns.weak) #16.74, 16.72 (evidence for H0)
bf.l.ns.inf <- bayes_factor(null.large.ns,l.ns.inf) #6392.59, 6360.84 (evidence for H0)
bf.l.ns.hyper <- bayes_factor(null.large.ns,l.ns.hyper) #>10,000, >10,000 (evidence for H0)

#Large s data
bf.l.s.flat <- lmBF(vk~age,df.s.large) #5.85 +- .01% (evidence for H1)
bf.l.s.diffuse <- bayes_factor(null.large.s.diffuse,l.s.diffuse) #<.0001,<.0001 (evidence for H0)
bf.l.s.weak <- bayes_factor(null.large.s,l.s.weak) #<.0001,<.0001 (evidence for H0)
bf.l.s.inf <- bayes_factor(null.large.s,l.s.inf) #<.0001,<.0001 (evidence for H0)
bf.l.s.hyper <- bayes_factor(null.large.s,l.s.hyper) #>10000,>10000 (evidence for H0)

```

Now that we've looked at the bayes factors for these models if our priors suggest an effect, let's look at them if our priors suggest there's not an effect. We're doing this because the BFs were pretty wonky in some spots above because I set a prior that was ridiculous. When set one that is more reasonable, our results are less wonky. I'll run all the brms in one block here.

```{r run the brms with priors on 0, include=FALSE}
#Small NS
ns.s.diffuseN <- brm(vk~age,data=df.ns.small,family=gaussian(),chains=4,cores=2,prior=diffuseN,sample_prior = TRUE,save_pars=save_pars(all=TRUE))

ns.s.weakN <- brm(vk~age,data=df.ns.small,family=gaussian(),chains=4,cores=2,prior=weak,sample_prior = TRUE,save_pars=save_pars(all=TRUE))

ns.s.infN <- brm(vk~age,data=df.ns.small,family=gaussian(),chains=4,cores=2,prior=informativeN,sample_prior = TRUE,save_pars=save_pars(all=TRUE))

ns.s.hyperN <- brm(vk~age,data=df.ns.small,family=gaussian(),chains=4,cores=2,prior=hyperinformativeN,sample_prior = TRUE,save_pars=save_pars(all=TRUE))

#Small Sig
s.s.diffuseN <- brm(vk~age,data=df.s.small,family=gaussian(),chains=4,cores=2,prior=diffuse.sN,sample_prior = TRUE,save_pars=save_pars(all=TRUE))

s.s.weakN <- brm(vk~age,data=df.s.small,family=gaussian(),chains=4,cores=2,prior=weakN,sample_prior = TRUE,save_pars=save_pars(all=TRUE))

s.s.infN <- brm(vk~age,data=df.s.small,family=gaussian(),chains=4,cores=2,prior=informativeN,sample_prior = TRUE,save_pars=save_pars(all=TRUE))

s.s.hyperN <- brm(vk~age,data=df.s.small,family=gaussian(),chains=4,cores=2,prior=hyperinformativeN,sample_prior = TRUE,save_pars=save_pars(all=TRUE))

#Large NS
l.ns.diffuseN <- brm(vk~age,data=df.ns.large,family=gaussian(),chains=4,cores=2,prior=diffuseN,sample_prior = TRUE,save_pars=save_pars(all=TRUE))

l.ns.weakN <- brm(vk~age,data=df.ns.large,family=gaussian(),chains=4,cores=2,prior=weakN,sample_prior = TRUE,save_pars=save_pars(all=TRUE))

l.ns.infN <- brm(vk~age,data=df.ns.large,family=gaussian(),chains=4,cores=2,prior=informativeN,sample_prior = TRUE,save_pars=save_pars(all=TRUE))

l.ns.hyperN <- brm(vk~age,data=df.ns.large,family=gaussian(),chains=4,cores=2,prior=hyperinformativeN,sample_prior = TRUE,save_pars=save_pars(all=TRUE))
#Large Sig
l.s.diffuseN <- brm(vk~age,data=df.s.large,family=gaussian(),chains=4,cores=2,prior=diffuse.sN,sample_prior = TRUE,save_pars=save_pars(all=TRUE))

l.s.weakN <- brm(vk~age,data=df.s.large,family=gaussian(),chains=4,cores=2,prior=weakN,sample_prior = TRUE,save_pars=save_pars(all=TRUE))

l.s.infN <- brm(vk~age,data=df.s.large,family=gaussian(),chains=4,cores=2,prior=informativeN,sample_prior = TRUE,save_pars=save_pars(all=TRUE))

l.s.hyperN <- brm(vk~age,data=df.s.large,family=gaussian(),chains=4,cores=2,prior=hyperinformativeN,sample_prior = TRUE,save_pars=save_pars(all=TRUE))
```

```{r output for null priors}
plot(hypothesis(ns.s.diffuseN,"age=0"),plot=FALSE)[[1]]+labs(title="Small, NS, Diffuse Prior")+xlim(-1.5,1.5)
plot(hypothesis(ns.s.weakN,"age=0"),plot=FALSE)[[1]]+labs(title="Small, NS, Weak Prior")+xlim(-1.5,1.5)
plot(hypothesis(ns.s.infN,"age=0"),plot=FALSE)[[1]]+labs(title="Small, NS, Inf Prior")+xlim(-1.5,1.5)
plot(hypothesis(ns.s.hyperN,"age=0"),plot=FALSE)[[1]]+labs(title="Small, NS, Hyper Prior")+xlim(-1.5,1.5)
plot(hypothesis(s.s.diffuseN,"age=0"),plot=FALSE)[[1]]+labs(title="Small, Sig, Diffuse Prior")+xlim(-1.5,1.5)
plot(hypothesis(s.s.weakN,"age=0"),plot=FALSE)[[1]]+labs(title="Small, Sig, Weak Prior")+xlim(-1.5,1.5)
plot(hypothesis(s.s.infN,"age=0"),plot=FALSE)[[1]]+labs(title="Small, Sig, Inf Prior")+xlim(-1.5,1.5)
plot(hypothesis(s.s.hyperN,"age=0"),plot=FALSE)[[1]]+labs(title="Small, Sig, Hyper Prior")+xlim(-1.5,1.5)
plot(hypothesis(l.ns.diffuseN,"age=0"),plot=FALSE)[[1]]+labs(title="Large, NS, Diffuse Prior")+xlim(-1.5,1.5)
plot(hypothesis(l.ns.weakN,"age=0"),plot=FALSE)[[1]]+labs(title="Large, NS, Weak Prior")+xlim(-1.5,1.5)
plot(hypothesis(l.ns.infN,"age=0"),plot=FALSE)[[1]]+labs(title="Large, NS, Inf Prior")+xlim(-1.5,1.5)
plot(hypothesis(l.ns.hyperN,"age=0"),plot=FALSE)[[1]]+labs(title="Large, NS, Hyper Prior")+xlim(-1.5,1.5)
plot(hypothesis(l.s.diffuseN,"age=0"),plot=FALSE)[[1]]+labs(title="Large, Sig, Diffuse Prior")+xlim(-1.5,1.5)
plot(hypothesis(l.s.weakN,"age=0"),plot=FALSE)[[1]]+labs(title="Large, Sig, Weak Prior")+xlim(-1.5,1.5)
plot(hypothesis(l.s.infN,"age=0"),plot=FALSE)[[1]]+labs(title="Large, Sig, Inf Prior")+xlim(-1.5,1.5)
plot(hypothesis(l.s.hyperN,"age=0"),plot=FALSE)[[1]]+labs(title="Large, Sig, Hyper Prior")+xlim(-1.5,1.5)
```
And finally, compute those BF. the BF on the flat prior won't change, so I skipped that step.
```{r compute bayes factors with priors suggesting the null}
#Small n.s. data
bf.s.ns.diffuseN <- bayes_factor(null.small.ns.diffuse,ns.s.diffuseN) #45.46, 45.60
bf.s.ns.weakN <- bayes_factor(null.small.ns,ns.s.weakN) #5.16, 5.12
bf.s.ns.infN <- bayes_factor(null.small.ns,ns.s.infN) #1.09, 1.1 
bf.s.ns.hyperN <- bayes_factor(null.small.ns,ns.s.hyperN) #1,1.001

#small sig data
bf.s.s.diffuseN <- bayes_factor(null.small.s.diffuse,s.s.diffuseN) #.04, .044
bf.s.s.weakN <- bayes_factor(null.small.s,s.s.weakN) #.0048, .0044
bf.s.s.infN <- bayes_factor(null.small.s,s.s.infN) #.0005, .0005
bf.s.s.hyperN <- bayes_factor(null.small.s,s.s.hyperN) #.00009, .03

#large ns data 
bf.l.ns.diffuseN <- bayes_factor(null.large.ns.diffuse,l.ns.diffuseN) #150.48, 150.38
bf.l.ns.weakN <- bayes_factor(null.large.ns,l.ns.weakN) #14.98, 15.07
bf.l.ns.infN <- bayes_factor(null.large.ns,l.ns.infN) #1.75, 1.79
bf.l.ns.hyperN <- bayes_factor(null.large.ns,l.ns.hyperN) #1.01, 1.01

#Large s data
bf.l.s.diffuseN <- bayes_factor(null.large.s.diffuse,l.s.diffuseN) #<.0001,<.0001
bf.l.s.weakN <- bayes_factor(null.large.s,l.s.weakN) #<.0001,<.0001
bf.l.s.infN <- bayes_factor(null.large.s,l.s.infN) #<.0001,<.0001
bf.l.s.hyperN <- bayes_factor(null.large.s,l.s.hyperN) #<.0001,<.0001

```

